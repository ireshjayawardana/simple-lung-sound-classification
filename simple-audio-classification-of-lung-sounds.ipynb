{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Copyright 2020 The TensorFlow Authors.","metadata":{"id":"fluF3_oOgkWF"}},{"cell_type":"markdown","source":"\n","metadata":{"id":"CNbqmZy0gbyE"}},{"cell_type":"markdown","source":"## Setup\n\nImport necessary modules and dependencies.\n","metadata":{"id":"Go9C3uLL8Izc"}},{"cell_type":"code","source":"import os\nimport pathlib\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom IPython import display\n\n\nimport seaborn as sns\nimport librosa.display\n\n# Set the seed value for experiment reproducibility.\n#seed = 42\n#tf.random.set_seed(seed)\n#np.random.seed(seed)\n\n","metadata":{"id":"dzLKpmZICaWN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_io as tfio\nimport soundfile\nimport statistics\nimport librosa as lb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"read the patient's details from the .csv file","metadata":{}},{"cell_type":"code","source":"patient_data=pd.read_csv('/kaggle/input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/patient_diagnosis.csv',names=['pid','disease'])\npatient_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"plot each disease with the number of recordings in the data set","metadata":{}},{"cell_type":"code","source":"sns.countplot(patient_data.disease)\nplt.xticks(rotation=90)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"get breath times and periods from files","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/160_1b3_Al_mc_AKGC417L.txt',sep='\\t')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"get the dataset path and file names of each racoding","metadata":{}},{"cell_type":"code","source":"path='/kaggle/input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/'\nDATASET_PATH = '/kaggle/input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/'\nfiles=[s.split('.')[0] for s in os.listdir(path) if '.txt' in s]\nfiles[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"create folder to put processed audio files","metadata":{}},{"cell_type":"code","source":"#save_path='processed_audio_files/' + filename\nfolder_path = 'processed_audio_files'\nif not os.path.exists(folder_path):\n    os.makedirs('processed_audio_files')","metadata":{"id":"d16bb8416f90","outputId":"51c48430-ef62-49c8-fa15-adfe3da23c33","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"get the file name spilt by the '_' and put into a array","metadata":{}},{"cell_type":"code","source":"def getFilenameInfo(file):\n    return file.split('_')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"to process all files in the data set 24bit -> 16 bit pcm\n\nadd the disease label on the front of each racording","metadata":{}},{"cell_type":"code","source":"files_data=[]\nfor file in files:\n    data=pd.read_csv(path + file + '.txt',sep='\\t',names=['start','end','crackles','weezels'])\n    name_data=getFilenameInfo(file)\n    data['pid']=name_data[0]\n    data['mode']=name_data[-2]\n    data['filename']=file\n    files_data.append(data)\nfiles_df=pd.concat(files_data)\nfiles_df.reset_index()\nfiles_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient_data.pid=patient_data.pid.astype('int32')\nfiles_df.pid=files_df.pid.astype('int32')\n\ndata=pd.merge(files_df,patient_data,on='pid')\ndata.head()\nif not os.path.exists('csv_data'):\n    os.makedirs('csv_data')\n    data.to_csv('csv_data/data.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"function to extract breath cycle from each recording according to the start and end time","metadata":{}},{"cell_type":"code","source":"\n\ndef getPureSample(raw_data,start,end,sr=22050):\n    '''\n    Takes a numpy array and spilts its using start and end args\n    \n    raw_data=numpy array of audio sample\n    start=time\n    end=time\n    sr=sampling_rate\n    mode=mono/stereo\n    \n    '''\n    max_ind = len(raw_data) \n    start_ind = min(int(start * sr), max_ind)\n    end_ind = min(int(end * sr), max_ind)\n    return raw_data[start_ind: end_ind]\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"plot breath duration(seconds) with Paitiant ID","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(x=(data.end-data.start), y=data.pid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"getaverage breath cycle","metadata":{}},{"cell_type":"code","source":"breath_length = data.end-data.start\naverage_length = statistics.mean(breath_length)\nprint(average_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#type(breath_length)\ntotal_no_of_breaths = breath_length.count()\nfor i in range (int(average_length) , 16):\n    count_less_than_i = 0\n    for j in range (total_no_of_breaths):\n        if (breath_length[j]<i):\n            count_less_than_i = count_less_than_i+1\n    percentage = count_less_than_i/total_no_of_breaths * 100\n    print(\"duration \" ,i , \"percentage of samples below that\" , percentage)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"98% of breathing cycles are below 6s","metadata":{}},{"cell_type":"code","source":"'''\nnew_samplerate = 16000\nfolder_path = 'processed_audio_files'\nc = 0\n\nfor i in files:\n    filename = i\n    data, samplerate = soundfile.read(DATASET_PATH+filename+'.wav')\n   # print(samplerate)\n    file_name_data = getFilenameInfo(filename)\n    for pid,disease in patient_data.iterrows():\n        if disease[0] == int(file_name_data[0]):\n            soundfile.write(folder_path + '/' + disease[1] + '_' + filename +'.wav', data, new_samplerate, subtype='PCM_16')\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i,c=0,0\nnew_samplerate = 16000\nfor index,row in data.iterrows():\n    maxLen=6\n    start=row['start']\n    end=row['end']\n    filename=row['filename']\n    \n    #get patient data\n    file_name_data = getFilenameInfo(filename)\n    for pid,disease in patient_data.iterrows():\n        if disease[0] == int(file_name_data[0]):\n            patients_disease = disease[1]\n            break\n            \n    #If len > maxLen , change it to maxLen\n    if end-start>maxLen:\n        end=start+maxLen\n    \n    audio_file_loc=DATASET_PATH + filename + '.wav'\n    \n    \n    if index > 0:\n        #check if more cycles exits for same patient if so then add i to change filename\n        if data.iloc[index-1]['filename']==filename:\n            i+=1\n        else:\n            i=0\n    filename= patients_disease+'_' + filename + '_' + str(i) + '.wav'\n    \n    save_path='processed_audio_files/' + filename\n    c+=1\n    \n    audioArr,sampleRate=lb.load(audio_file_loc)\n    pureSample=getPureSample(audioArr,start,end,sampleRate)\n    \n    #pad audio if pureSample len < max_len\n    reqLen=6*sampleRate\n    padded_data = lb.util.pad_center(pureSample, reqLen)\n    \n    soundfile.write(file=save_path,data=padded_data,samplerate=new_samplerate)\nprint('Total Files Processed: ',c)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"read new filenames","metadata":{}},{"cell_type":"code","source":"audio_data_dir = \"processed_audio_files/*.wav\"\n\nfilenames = tf.io.gfile.glob(audio_data_dir)\n\n#filenames = tf.random.shuffle(filenames)\nnum_samples = len(filenames)\nprint('Number of total examples:', num_samples)\nprint (filenames[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"split the data set into 3 parts - > train / test / validate","metadata":{}},{"cell_type":"code","source":"train_files = filenames[:736]\nval_files = filenames[736: 828 ]\ntest_files = filenames[-92:]\n\nprint('Training set size', len(train_files))\nprint('Validation set size', len(val_files))\nprint('Test set size', len(test_files))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"labels of the data set with patient conditions","metadata":{}},{"cell_type":"code","source":"disease_labels = np.array([\"asthma\",\"COPD\",\"Bronchiectasis\",\"Pneumonia\",\"URTI\",\"healthy\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"function that preprocesses the dataset's raw WAV audio files into audio tensors:","metadata":{"id":"e6bb8defd2ef"}},{"cell_type":"code","source":"def decode_audio(audio_binary):\n  # Decode WAV-encoded audio files to `float32` tensors, normalized\n  # to the [-1.0, 1.0] range. Return `float32` audio and a sample rate.\n  audio, _ = tf.audio.decode_wav(contents=audio_binary)\n  # Since all the data is single channel (mono), drop the `channels`\n  # axis from the array.\n  return tf.squeeze(audio, axis=-1)","metadata":{"id":"9PjJ2iXYwftD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define a function that creates labels using the parent directories for each file:\n\n- Split the file paths into `tf.RaggedTensor`s (tensors with ragged dimensions—with slices that may have different lengths).","metadata":{"id":"GPQseZElOjVN"}},{"cell_type":"code","source":"def get_label(file_path):\n  parts = tf.strings.split(\n      input=file_path,\n      sep=os.path.sep)\n  # Note: You'll use indexing here instead of tuple unpacking to enable this\n  # to work in a TensorFlow graph.\n  return parts[-1]","metadata":{"id":"8VTtX1nr3YT-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define another helper function—`get_waveform_and_label`—that puts it all together:\n\n- The input is the WAV audio filename.\n- The output is a tuple containing the audio and label tensors ready for supervised learning.","metadata":{"id":"E8Y9w_5MOsr-"}},{"cell_type":"code","source":"file_name_data = getFilenameInfo('160_1b3_Al_mc_AKGC417L')\n#print(int(file_name_data[0]))\n\nfor pid,disease in patient_data.iterrows():\n    #print(type(disease[0]))\n    if disease[0] == int(file_name_data[0]):\n        print (disease[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_waveform_and_label(file_path):\n  label = get_label(file_path)\n  print (label)\n  parts = tf.strings.split(\n      input=label,\n      sep='_')\n\n  label = parts[0]   # contains a tensor with pids\n  \n  audio_binary = tf.io.read_file(file_path)\n\n  waveform = decode_audio(audio_binary)\n  return waveform, label","metadata":{"id":"WdgUD5T93NyT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Build the training set to extract the audio-label pairs:\n\n- Create a `tf.data.Dataset` with `Dataset.from_tensor_slices` and `Dataset.map`, using `get_waveform_and_label` defined earlier.\n\nYou'll build the validation and test sets using a similar procedure later on.","metadata":{"id":"nvN8W_dDjYjc"}},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\n\nfiles_ds = tf.data.Dataset.from_tensor_slices(train_files)\n\nwaveform_ds = files_ds.map(\n    map_func=get_waveform_and_label,\n    num_parallel_calls=AUTOTUNE)","metadata":{"id":"0SQl8yXl3kNP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's plot a few audio waveforms:","metadata":{"id":"voxGEwvuh2L7"}},{"cell_type":"code","source":"rows = 3\ncols = 3\nn = rows * cols\nfig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n\nfor i, (audio, label) in enumerate(waveform_ds.take(n)):\n  r = i // cols\n  c = i % cols\n  ax = axes[r][c]\n  ax.plot(audio.numpy())\n  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n  label = label.numpy().decode('utf-8')\n  ax.set_title(label)\n\nplt.show()","metadata":{"id":"8yuX6Nqzf6wT","outputId":"ae0b4199-2e8b-4564-bc33-f201993dd1ab","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## transform to spectrograms -  > STFT\n","metadata":{"id":"EWXPphxm0B4m"}},{"cell_type":"code","source":"def get_spectrogram(waveform):\n  # Zero-padding for an audio waveform with less than 16,000 samples.\n  input_len = 16000\n  waveform = waveform[:input_len]\n  zero_padding = tf.zeros(\n      [16000] - tf.shape(waveform),\n      dtype=tf.float32)\n  # Cast the waveform tensors' dtype to float32.\n  waveform = tf.cast(waveform, dtype=tf.float32)\n  # Concatenate the waveform with `zero_padding`, which ensures all audio\n  # clips are of the same length.\n  equal_length = tf.concat([waveform, zero_padding], 0)\n  # Convert the waveform to a spectrogram via a STFT.\n  spectrogram = tf.signal.stft(\n      equal_length, frame_length=255, frame_step=128)\n  # Obtain the magnitude of the STFT.\n  spectrogram = tf.abs(spectrogram)\n  # Add a `channels` dimension, so that the spectrogram can be used\n  # as image-like input data with convolution layers (which expect\n  # shape (`batch_size`, `height`, `width`, `channels`).\n  spectrogram = spectrogram[..., tf.newaxis]\n  return spectrogram","metadata":{"id":"_4CK75DHz_OR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Next, start exploring the data. Print the shapes of one example's tensorized waveform and the corresponding spectrogram, and play the original audio:","metadata":{"id":"5rdPiPYJphs2"}},{"cell_type":"code","source":"for waveform, label in waveform_ds.take(1):\n  label = label.numpy().decode('utf-8')\n  spectrogram = get_spectrogram(waveform)\n\nprint('Label:', label)\nprint('Waveform shape:', waveform.shape)\nprint('Spectrogram shape:', spectrogram.shape)\nprint('Audio playback')\ndisplay.display(display.Audio(waveform, rate=16000))","metadata":{"id":"4Mu6Y7Yz3C-V","outputId":"279075e6-84c1-4448-ecc1-6e43b469e9f1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, define a function for displaying a spectrogram:","metadata":{"id":"xnSuqyxJ1isF"}},{"cell_type":"code","source":"def plot_spectrogram(spectrogram, ax):\n  if len(spectrogram.shape) > 2:\n    assert len(spectrogram.shape) == 3\n    spectrogram = np.squeeze(spectrogram, axis=-1)\n  # Convert the frequencies to log scale and transpose, so that the time is\n  # represented on the x-axis (columns).\n  # Add an epsilon to avoid taking a log of zero.\n  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n  height = log_spec.shape[0]\n  width = log_spec.shape[1]\n  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n  Y = range(height)\n  ax.pcolormesh(X, Y, log_spec)","metadata":{"id":"e62jzb36-Jog","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the example's waveform over time and the corresponding spectrogram (frequencies over time):","metadata":{"id":"baa5c91e8603"}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, figsize=(12, 8))\ntimescale = np.arange(waveform.shape[0])\naxes[0].plot(timescale, waveform.numpy())\naxes[0].set_title('Waveform')\naxes[0].set_xlim([0, 16000])\n\nplot_spectrogram(spectrogram.numpy(), axes[1])\naxes[1].set_title('Spectrogram')\nplt.show()","metadata":{"id":"d2_CikgY1tjv","outputId":"490abc21-8190-43b7-bc73-2c3965b5778d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, define a function that transforms the waveform dataset into spectrograms and their corresponding labels as integer IDs:","metadata":{"id":"GyYXjW07jCHA"}},{"cell_type":"code","source":"def get_spectrogram_and_label_id(audio, label):\n  spectrogram = get_spectrogram(audio)\n  label_id = tf.argmax(label == disease_labels)\n  \n  return spectrogram, label_id","metadata":{"id":"43IS2IouEV40","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Map `get_spectrogram_and_label_id` across the dataset's elements with `Dataset.map`:","metadata":{"id":"cf5d5b033a45"}},{"cell_type":"code","source":"spectrogram_ds = waveform_ds.map(map_func=get_spectrogram_and_label_id,num_parallel_calls=AUTOTUNE)\n","metadata":{"id":"yEVb_oK0oBLQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Examine the spectrograms for different examples of the dataset:","metadata":{"id":"6gQpAAgMnyDi"}},{"cell_type":"code","source":"rows = 3\ncols = 3\nn = rows*cols\nfig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n\nfor i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n  r = i // cols\n  c = i % cols\n  ax = axes[r][c]\n  plot_spectrogram(spectrogram.numpy(), ax)\n  ax.set_title(disease_labels[label_id.numpy()])\n  \n  ax.axis('off')\n  \nplt.show()","metadata":{"id":"QUbHfTuon4iF","outputId":"d9bc4ba9-81f1-46ee-d7c3-9cb173b10bf7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MFCC Spectrogram","metadata":{}},{"cell_type":"code","source":"\n\nsample_file = 'processed_audio_files/COPD_172_1b3_Pl_mc_AKGC417L.wav'\n\nimport librosa.display\n#file=path + Xtrain.iloc[193].filename \nsound,sample_rate=librosa.load(sample_file)\nmfccs = librosa.feature.mfcc(y=sound, sr=sample_rate, n_mfcc=40)\nfig, ax = plt.subplots()\nimg = librosa.display.specshow(mfccs, x_axis='time', ax=ax)\nfig.colorbar(img, ax=ax)\nax.set(title='MFCC')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build and train the model\n\nRepeat the training set preprocessing on the validation and test sets:","metadata":{"id":"z5KdY8IF8rkt"}},{"cell_type":"code","source":"def preprocess_dataset(files):\n  files_ds = tf.data.Dataset.from_tensor_slices(files)\n  output_ds = files_ds.map(\n      map_func=get_waveform_and_label,\n      num_parallel_calls=AUTOTUNE)\n  output_ds = output_ds.map(\n      map_func=get_spectrogram_and_label_id,\n      num_parallel_calls=AUTOTUNE)\n  return output_ds","metadata":{"id":"10UI32QH_45b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = spectrogram_ds\nval_ds = preprocess_dataset(val_files)\ntest_ds = preprocess_dataset(test_files)","metadata":{"id":"HNv4xwYkB2P6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Batch the training and validation sets for model training:","metadata":{"id":"assnWo6SB3lR"}},{"cell_type":"code","source":"batch_size = 64\ntrain_ds = train_ds.batch(batch_size)\nval_ds = val_ds.batch(batch_size)","metadata":{"id":"UgY9WYzn61EX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add `Dataset.cache` and `Dataset.prefetch` operations to reduce read latency while training the model:","metadata":{"id":"GS1uIh6F_TN9"}},{"cell_type":"code","source":"train_ds = train_ds.cache().prefetch(AUTOTUNE)\nval_ds = val_ds.cache().prefetch(AUTOTUNE)","metadata":{"id":"fdZ6M-F5_QzY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the model, you'll use a simple convolutional neural network (CNN), since you have transformed the audio files into spectrogram images.\n\nYour `tf.keras.Sequential` model will use the following Keras preprocessing layers:\n\n- `tf.keras.layers.Resizing`: to downsample the input to enable the model to train faster.\n- `tf.keras.layers.Normalization`: to normalize each pixel in the image based on its mean and standard deviation.\n\nFor the `Normalization` layer, its `adapt` method would first need to be called on the training data in order to compute aggregate statistics (that is, the mean and the standard deviation).","metadata":{"id":"rwHkKCQQb5oW"}},{"cell_type":"code","source":"for spectrogram, _ in spectrogram_ds.take(1):\n  input_shape = spectrogram.shape\nprint('Input shape:', input_shape)\nnum_labels = len(disease_labels)\n\n# Instantiate the `tf.keras.layers.Normalization` layer.\nnorm_layer = layers.Normalization()\n# Fit the state of the layer to the spectrograms\n# with `Normalization.adapt`.\nnorm_layer.adapt(data=spectrogram_ds.map(map_func=lambda spec, label: spec))\n\nmodel = models.Sequential([\n    layers.Input(shape=input_shape),\n    # Downsample the input.\n    layers.Resizing(32, 32),\n    # Normalize.\n    norm_layer,\n    layers.Conv2D(32, 3, activation='relu'),\n    layers.Conv2D(64, 3, activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.25),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(num_labels),\n])\n\nmodel.summary()","metadata":{"id":"ALYz7PFCHblP","outputId":"e21c6b0e-d869-4032-9edf-bf3e9b21d76b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Configure the Keras model with the Adam optimizer and the cross-entropy loss:","metadata":{"id":"de52e5afa2f3"}},{"cell_type":"code","source":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy'],\n)","metadata":{"id":"wFjj7-EmsTD-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train the model over 10 epochs for demonstration purposes:","metadata":{"id":"f42b9e3a4705"}},{"cell_type":"code","source":"EPOCHS = 10\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS,\n    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n)","metadata":{"id":"ttioPJVMcGtq","outputId":"97b3c1c9-9e00-463b-c18a-140d74331780","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's plot the training and validation loss curves to check how your model has improved during training:","metadata":{"id":"gjpCDeQ4mUfS"}},{"cell_type":"code","source":"metrics = history.history\nplt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\nplt.legend(['loss', 'val_loss'])\nplt.show()","metadata":{"id":"nzhipg3Gu2AY","outputId":"20b97662-c8b5-4ee7-87a4-1db409bd8881","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate the model performance\n\nRun the model on the test set and check the model's performance:","metadata":{"id":"5ZTt3kO3mfm4"}},{"cell_type":"code","source":"test_audio = []\ntest_labels = []\n\nfor audio, label in test_ds:\n  test_audio.append(audio.numpy())\n  test_labels.append(label.numpy())\n\ntest_audio = np.array(test_audio)\ntest_labels = np.array(test_labels)","metadata":{"id":"biU2MwzyAo8o","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = np.argmax(model.predict(test_audio), axis=1)\ny_true = test_labels\n\ntest_acc = sum(y_pred == y_true) / len(y_true)\nprint(f'Test set accuracy: {test_acc:.0%}')","metadata":{"id":"ktUanr9mRZky","outputId":"9049afdc-6311-4349-f99d-2b14144cabdb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Display a confusion matrix\n\nUse a <a href=\"https://developers.google.com/machine-learning/glossary#confusion-matrix\" class=\"external\">confusion matrix</a> to check how well the model did classifying each of the commands in the test set:\n","metadata":{"id":"en9Znt1NOabH"}},{"cell_type":"code","source":"confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(10, 8))\nsns.heatmap(confusion_mtx,\n            xticklabels=disease_labels,\n            yticklabels=disease_labels,\n            annot=True, fmt='g')\nplt.xlabel('Prediction')\nplt.ylabel('Label')\nplt.show()","metadata":{"id":"LvoSAOiXU3lL","outputId":"29f065d0-b810-4fdb-8894-9b125bd85936","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run inference on an audio file\n\nFinally, verify the model's prediction output using an input audio file of someone saying \"no\". How well does your model perform?","metadata":{"id":"mQGi_mzPcLvl"}},{"cell_type":"code","source":"sample_file = 'processed_audio_files/COPD_172_1b3_Pl_mc_AKGC417L.wav'\n\nsample_ds = preprocess_dataset([str(sample_file)])\n\nfor spectrogram, label in sample_ds.batch(1):\n  prediction = model(spectrogram)\n  plt.bar(disease_labels, tf.nn.softmax(prediction[0]))\n  plt.title(f'Predictions for \"{disease_labels[label[0]]}\"')\n  plt.show()","metadata":{"id":"zRxauKMdhofU","outputId":"da09c36e-15b8-498c-e1f8-dfc5f06c0d2b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the output suggests, your model should have recognized the audio command as \"no\".","metadata":{"id":"VgWICqdqQNaQ"}},{"cell_type":"markdown","source":"## Next steps\n\nThis tutorial demonstrated how to carry out simple audio classification/automatic speech recognition using a convolutional neural network with TensorFlow and Python. To learn more, consider the following resources:\n\n- The [Sound classification with YAMNet](https://www.tensorflow.org/hub/tutorials/yamnet) tutorial shows how to use transfer learning for audio classification.\n- The notebooks from <a href=\"https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/overview\" class=\"external\">Kaggle's TensorFlow speech recognition challenge</a>.\n- The \n<a href=\"https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0\" class=\"external\">TensorFlow.js - Audio recognition using transfer learning codelab</a> teaches how to build your own interactive web app for audio classification.\n- <a href=\"https://arxiv.org/abs/1709.04396\" class=\"external\">A tutorial on deep learning for music information retrieval</a> (Choi et al., 2017) on arXiv.\n- TensorFlow also has additional support for [audio data preparation and augmentation](https://www.tensorflow.org/io/tutorials/audio) to help with your own audio-based projects.\n- Consider using the <a href=\"https://librosa.org/\" class=\"external\">librosa</a> library—a Python package for music and audio analysis.","metadata":{"id":"J3jF933m9z1J"}}]}